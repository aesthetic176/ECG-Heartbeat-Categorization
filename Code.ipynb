{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Code**"
      ],
      "metadata": {
        "id": "n99GMBiEHNqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load datasets\n",
        "ptbdb_normal = pd.read_csv('ptbdb_normal.csv', header=None)\n",
        "ptbdb_abnormal = pd.read_csv('ptbdb_abnormal.csv', header=None)\n",
        "mitbih_train = pd.read_csv('mitbih_train.csv', header=None)\n",
        "mitbih_test = pd.read_csv('mitbih_test.csv', header=None)\n",
        "\n",
        "# Concatenate and preprocess data\n",
        "ptbdb = pd.concat([ptbdb_normal, ptbdb_abnormal])\n",
        "mitbih = pd.concat([mitbih_train, mitbih_test])\n",
        "\n",
        "# Combine datasets\n",
        "X = pd.concat([ptbdb.iloc[:, :-1], mitbih.iloc[:, :-1]]).values\n",
        "y = pd.concat([ptbdb.iloc[:, -1], mitbih.iloc[:, -1]]).values\n",
        "\n",
        "# Check for NaN values and handle them\n",
        "if np.isnan(X).any():\n",
        "    X = np.nan_to_num(X)\n",
        "\n",
        "if np.isnan(y).any():\n",
        "    y = np.nan_to_num(y)\n",
        "\n",
        "# Correct labels that are not 0 or 1\n",
        "y[y != 0] = 1\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape data for CNN\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Train-test split with shuffling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Ensure labels are integers\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "# Convert labels to categorical\n",
        "num_classes = 2\n",
        "y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# CNN Model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile model with a lower learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgrKB-0hBOp2",
        "outputId": "72890a27-c329-41ea-8567-f03f490c7108"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "872/872 [==============================] - 42s 46ms/step - loss: 0.3644 - accuracy: 0.8543 - val_loss: 0.2334 - val_accuracy: 0.9030\n",
            "Epoch 2/30\n",
            "872/872 [==============================] - 42s 49ms/step - loss: 0.2386 - accuracy: 0.9072 - val_loss: 0.1710 - val_accuracy: 0.9330\n",
            "Epoch 3/30\n",
            "872/872 [==============================] - 42s 48ms/step - loss: 0.1888 - accuracy: 0.9285 - val_loss: 0.1431 - val_accuracy: 0.9499\n",
            "Epoch 4/30\n",
            "872/872 [==============================] - 39s 45ms/step - loss: 0.1575 - accuracy: 0.9413 - val_loss: 0.1217 - val_accuracy: 0.9589\n",
            "Epoch 5/30\n",
            "872/872 [==============================] - 39s 45ms/step - loss: 0.1393 - accuracy: 0.9490 - val_loss: 0.1096 - val_accuracy: 0.9643\n",
            "Epoch 6/30\n",
            "872/872 [==============================] - 40s 46ms/step - loss: 0.1229 - accuracy: 0.9565 - val_loss: 0.0953 - val_accuracy: 0.9679\n",
            "Epoch 7/30\n",
            "872/872 [==============================] - 42s 48ms/step - loss: 0.1077 - accuracy: 0.9618 - val_loss: 0.0894 - val_accuracy: 0.9698\n",
            "Epoch 8/30\n",
            "872/872 [==============================] - 41s 47ms/step - loss: 0.0967 - accuracy: 0.9653 - val_loss: 0.0893 - val_accuracy: 0.9704\n",
            "Epoch 9/30\n",
            "872/872 [==============================] - 40s 46ms/step - loss: 0.0884 - accuracy: 0.9691 - val_loss: 0.0809 - val_accuracy: 0.9732\n",
            "Epoch 10/30\n",
            "872/872 [==============================] - 42s 48ms/step - loss: 0.0793 - accuracy: 0.9724 - val_loss: 0.0801 - val_accuracy: 0.9720\n",
            "Epoch 11/30\n",
            "872/872 [==============================] - 42s 49ms/step - loss: 0.0751 - accuracy: 0.9739 - val_loss: 0.0930 - val_accuracy: 0.9675\n",
            "Epoch 12/30\n",
            "872/872 [==============================] - 41s 47ms/step - loss: 0.0676 - accuracy: 0.9766 - val_loss: 0.0679 - val_accuracy: 0.9780\n",
            "Epoch 13/30\n",
            "872/872 [==============================] - 38s 44ms/step - loss: 0.0646 - accuracy: 0.9785 - val_loss: 0.0784 - val_accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "872/872 [==============================] - 39s 45ms/step - loss: 0.0587 - accuracy: 0.9802 - val_loss: 0.0646 - val_accuracy: 0.9802\n",
            "Epoch 15/30\n",
            "872/872 [==============================] - 40s 46ms/step - loss: 0.0556 - accuracy: 0.9817 - val_loss: 0.0609 - val_accuracy: 0.9810\n",
            "Epoch 16/30\n",
            "872/872 [==============================] - 40s 46ms/step - loss: 0.0519 - accuracy: 0.9835 - val_loss: 0.0608 - val_accuracy: 0.9806\n",
            "Epoch 17/30\n",
            "872/872 [==============================] - 40s 46ms/step - loss: 0.0473 - accuracy: 0.9842 - val_loss: 0.0586 - val_accuracy: 0.9822\n",
            "Epoch 18/30\n",
            "872/872 [==============================] - 38s 44ms/step - loss: 0.0459 - accuracy: 0.9848 - val_loss: 0.0582 - val_accuracy: 0.9826\n",
            "Epoch 19/30\n",
            "872/872 [==============================] - 39s 45ms/step - loss: 0.0426 - accuracy: 0.9861 - val_loss: 0.0562 - val_accuracy: 0.9813\n",
            "Epoch 20/30\n",
            "872/872 [==============================] - 42s 48ms/step - loss: 0.0412 - accuracy: 0.9861 - val_loss: 0.0558 - val_accuracy: 0.9831\n",
            "Epoch 21/30\n",
            "872/872 [==============================] - 39s 45ms/step - loss: 0.0376 - accuracy: 0.9875 - val_loss: 0.0609 - val_accuracy: 0.9818\n",
            "Epoch 22/30\n",
            "872/872 [==============================] - 42s 48ms/step - loss: 0.0357 - accuracy: 0.9879 - val_loss: 0.0626 - val_accuracy: 0.9816\n",
            "Epoch 23/30\n",
            "872/872 [==============================] - 42s 48ms/step - loss: 0.0344 - accuracy: 0.9886 - val_loss: 0.0626 - val_accuracy: 0.9812\n",
            "Epoch 24/30\n",
            "872/872 [==============================] - 39s 44ms/step - loss: 0.0322 - accuracy: 0.9892 - val_loss: 0.0578 - val_accuracy: 0.9827\n",
            "Epoch 25/30\n",
            "872/872 [==============================] - 39s 45ms/step - loss: 0.0311 - accuracy: 0.9900 - val_loss: 0.0727 - val_accuracy: 0.9789\n",
            "Epoch 26/30\n",
            "872/872 [==============================] - 40s 46ms/step - loss: 0.0290 - accuracy: 0.9905 - val_loss: 0.0549 - val_accuracy: 0.9848\n",
            "Epoch 27/30\n",
            "872/872 [==============================] - 41s 47ms/step - loss: 0.0260 - accuracy: 0.9918 - val_loss: 0.0548 - val_accuracy: 0.9841\n",
            "Epoch 28/30\n",
            "872/872 [==============================] - 40s 46ms/step - loss: 0.0265 - accuracy: 0.9905 - val_loss: 0.0563 - val_accuracy: 0.9853\n",
            "Epoch 29/30\n",
            "872/872 [==============================] - 42s 48ms/step - loss: 0.0258 - accuracy: 0.9912 - val_loss: 0.0562 - val_accuracy: 0.9836\n",
            "Epoch 30/30\n",
            "872/872 [==============================] - 39s 45ms/step - loss: 0.0247 - accuracy: 0.9915 - val_loss: 0.0604 - val_accuracy: 0.9824\n",
            "436/436 [==============================] - 4s 10ms/step - loss: 0.0604 - accuracy: 0.9824\n",
            "Test Accuracy: 98.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Explanation\n",
        "\n",
        "The above code is developed for binary classification, aimed at distinguishing between healthy (normal) and anomalous (abnormal) heartbeats using ECG signal data. This task is crucial in medical diagnostics, particularly in the detection and monitoring of heart conditions.\n",
        "\n",
        "The process begins with loading four separate datasets. Two of these datasets, labeled as 'ptbdb_normal' and 'ptbdb_abnormal', contain ECG signals representing normal and abnormal heartbeats, respectively. The other two datasets, 'mitbih_train' and 'mitbih_test', likely consist of a mixed collection of both normal and abnormal heartbeats.\n",
        "\n",
        "After loading the datasets, the code combines them into a single dataset, creating a more comprehensive collection for analysis. It then separates the ECG signals (features) and their corresponding labels (normal or abnormal) into 'X' and 'y' variables.\n",
        "\n",
        "An essential step in the preparation of the data involves handling missing or invalid values, known as NaNs (Not a Number), and ensuring all labels are correctly formatted as 0 (normal) or 1 (abnormal) for binary classification. This preparation is crucial for the accuracy and reliability of the classification model.\n",
        "\n",
        "The code then standardizes the ECG signals, a process that normalizes the data to ensure uniformity and comparability. This step is vital for effective model training and prediction accuracy.\n",
        "\n",
        "The next phase involves reshaping the standardized data to fit a Convolutional Neural Network (CNN) model. CNNs are highly effective in pattern recognition tasks, making them suitable for analyzing ECG signals. The data is divided into training and testing sets, allowing the model to learn from one set and validate its learning on the other.\n",
        "\n",
        "The model architecture includes Convolutional, MaxPooling, Flatten, Dropout, and Dense layers, structured to extract significant features from the ECG signals and classify them accurately. The model is compiled with an Adam optimizer and a lower learning rate to enhance training stability.\n",
        "\n",
        "After training the model with the specified number of epochs and batch size, its performance is evaluated using the testing data set. The accuracy metric provides insight into how well the model differentiates between normal and abnormal heartbeats.\n",
        "\n",
        "In conclusion, this code represents a comprehensive approach to using machine learning, particularly CNNs, for the critical task of identifying heart conditions through ECG signal analysis."
      ],
      "metadata": {
        "id": "-OnqKr69M1_K"
      }
    }
  ]
}